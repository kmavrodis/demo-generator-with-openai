{"id": "3fdb14fc-f8d5-4c17-9036-76190716417c", "use_case": "Video Analysis and commentary generation", "detailed_description": "**Demo Description: Video Analysis and Commentary Generation**\n\n**Main Features of the Demo:**\n- Upload video files for analysis.\n- Extract frames from the video for image analysis.\n- Generate descriptive commentary for each frame.\n- Display video frames alongside generated commentary.\n\n**User Interface Elements:**\n- File uploader to select and upload video files.\n- A button to initiate the analysis process.\n- A side-by-side display of video frames and corresponding AI-generated commentary.\n- Scrollable interface to navigate through frames and commentary.\n- A download option for saving the generated commentary as a text file.\n\n**Data Processing Steps:**\n1. **Video Upload:**\n   - User uploads a video file using the file uploader.\n   \n2. **Frame Extraction:**\n   - Extract frames from the video at regular intervals using Python libraries such as OpenCV.\n   \n3. **Image Analysis:**\n   - Use the Azure OpenAI Chat API with images attached to analyze each frame.\n   - Detect objects, activities, and emotions within each frame.\n   \n4. **Commentary Generation:**\n   - Generate descriptive commentary for each frame based on the image analysis using the Azure OpenAI Chat API for text generation.\n   \n5. **Display Results:**\n   - Display the video frames alongside the generated commentary in a scrollable Streamlit interface.\n   - Option to download the commentary as a text file for offline use.\n\n**Instructions for the Engineer:**\n- Ensure the interface is user-friendly with clear instructions.\n- Use dummy data for video frames and commentary if real-time processing is not feasible.\n- Incorporate visual elements like thumbnails for video frames and styled text for commentary.\n- Optimize the frame extraction and commentary generation for responsiveness.", "code": "import streamlit as st\nimport tempfile\nimport os\nimport cv2\nimport numpy as np\nfrom openai import AzureOpenAI\nfrom PIL import Image\nimport base64\n\n# Install the required packages if not available\ntry:\n    import cv2\nexcept ImportError:\n    os.system('pip install opencv-python-headless')\n\ntry:\n    import numpy as np\nexcept ImportError:\n    os.system('pip install numpy')\n\ntry:\n    from PIL import Image\nexcept ImportError:\n    os.system('pip install pillow')\n\ntry:\n    from openai import AzureOpenAI\nexcept ImportError:\n    os.system('pip install openai')\n\n# Initialize Azure OpenAI client\napi_key = os.getenv(\"OPENAI_API_KEY\")\nazure_endpoint = os.getenv(\"OPENAI_ENDPOINT\")\ndeployment_name = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n\nclient = AzureOpenAI(\n    api_key=api_key,\n    api_version=\"2023-12-01-preview\",\n    azure_endpoint=azure_endpoint\n)\n\n# Function to ensure use case specific directory exists\ndef ensure_directory(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n# Function to extract frames from video segments\ndef extract_frames_from_segment(video_path, start_time, duration, frames_per_segment):\n    vidcap = cv2.VideoCapture(video_path)\n    fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n    vidcap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)\n\n    frames = []\n    for _ in range(frames_per_segment):\n        success, image = vidcap.read()\n        if not success:\n            break\n        frames.append(image)\n        vidcap.set(cv2.CAP_PROP_POS_FRAMES, vidcap.get(cv2.CAP_PROP_POS_FRAMES) + fps * (duration / frames_per_segment))\n\n    vidcap.release()\n    return frames\n\n# Function to convert frame to base64 string\ndef frame_to_base64(frame):\n    _, buffer = cv2.imencode('.jpg', frame)\n    frame_base64 = base64.b64encode(buffer).decode('utf-8')\n    return frame_base64\n\n# Function to get image analysis and generate commentary\ndef analyze_segment(frames_base64):\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant and will generate a description of the images in this segment without explicitly mentioning the frames or images.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate a summary description of the content observed in this segment based on the provided images.\",\n        },\n    ]\n\n    for frame in frames_base64:\n        messages.append({\n            \"role\": \"user\",\n            \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame}\"}}],\n        })\n\n    response = client.chat.completions.create(\n        model=deployment_name,\n        messages=messages\n    )\n\n    return response.choices[0].message.content\n\n# Function to create SRT file\ndef create_srt(commentary, directory):\n    ensure_directory(directory)\n    srt_content = []\n    for idx, (start_time, description) in enumerate(commentary):\n        start_seconds = start_time % 60\n        start_minutes = (start_time // 60) % 60\n        start_hours = start_time // 3600\n        end_seconds = (start_time + segment_duration) % 60\n        end_minutes = ((start_time + segment_duration) // 60) % 60\n        end_hours = (start_time + segment_duration) // 3600\n        srt_content.append(f\"{idx + 1}\")\n        srt_content.append(f\"{start_hours:02}:{start_minutes:02}:{start_seconds:02},000 --> {end_hours:02}:{end_minutes:02}:{end_seconds:02},000\")\n        srt_content.append(description)\n        srt_content.append(\"\")  # SRT requires an empty line between entries\n\n    srt_file_path = os.path.join(directory, \"commentary.srt\")\n    with open(srt_file_path, 'w', encoding='utf-8') as f:\n        f.write(\"\\n\".join(srt_content))\n    \n    return srt_file_path\n\n# Streamlit User Interface\nst.title(\"Video Segment Analysis and Commentary Generation\")\nst.write(\"Upload a video file to extract frames from segments and generate descriptive commentary for each segment.\")\n\nuploaded_video = st.file_uploader(\"Upload Video\", type=[\"mp4\", \"avi\", \"mov\"])\nsegment_duration = st.slider(\"Select segment duration (seconds)\", min_value=1, max_value=60, value=10, step=1)\nframes_per_segment = st.slider(\"Select frames per segment\", min_value=1, max_value=10, value=2, step=1)\n\nif uploaded_video:\n    use_case_dir = \"data/video_analysis\"\n    ensure_directory(use_case_dir)\n    video_path = os.path.join(use_case_dir, uploaded_video.name)\n\n    with open(video_path, \"wb\") as f:\n        f.write(uploaded_video.read())\n\n    st.success(\"Video uploaded successfully. Click 'Analyze Video' to proceed.\")\n\n    if st.button('Analyze Video'):\n        st.info(\"Extracting frames and generating commentary...\")\n\n        vidcap = cv2.VideoCapture(video_path)\n        total_duration = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT) / vidcap.get(cv2.CAP_PROP_FPS))\n        vidcap.release()\n\n        commentary = []\n        for start_time in range(0, total_duration, segment_duration):\n            frames = extract_frames_from_segment(video_path, start_time, segment_duration, frames_per_segment)\n            frames_base64 = [frame_to_base64(frame) for frame in frames]\n            description = analyze_segment(frames_base64)\n            commentary.append((start_time, description))\n\n        st.success(\"Analysis complete. Scroll down to view the results.\")\n\n        # Display segments and commentary\n        for idx, (start_time, description) in enumerate(commentary):\n            st.write(f\"Segment {idx + 1} (starting at {start_time} seconds):\")\n            st.write(description)\n\n        # Create and display SRT file\n        srt_file_path = create_srt(commentary, use_case_dir)\n        with open(srt_file_path, 'rb') as srt_file:\n            srt_bytes = srt_file.read()\n\n        st.video(video_path, format=\"video/mp4\", subtitles=srt_file_path, start_time=0)"}